{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d684c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f4b703",
   "metadata": {},
   "source": [
    "# Disaster Tweets Analysis \n",
    "\n",
    "https://github.com/himansharma21/NLP\n",
    "\n",
    "We will attempt to create a model to predict if a given tweet was written about a natural disaster. \n",
    "\n",
    "\n",
    "https://www.kaggle.com/c/nlp-getting-started/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a187c3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    'train.csv',          \n",
    ")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9727aad",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Natural language data — particularly tweets — often requires extensive preprocessing due to its informal and inconsistent structure.  \n",
    "In this case, our cleaning steps will include:\n",
    "\n",
    "- Converting all text to **lowercase** to ensure uniformity.\n",
    "- **Removing mentions** (e.g., `@username`), which typically do not carry meaningful information for our modeling task.\n",
    "- **Eliminating extra whitespace** to standardize the formatting.\n",
    "\n",
    "These cleaning steps help simplify the text, reduce noise, and ensure that the model focuses on the most relevant linguistic content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9210d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pattern = r\"http\\S+|www\\S+|https\\S+\"\n",
    "mention_pattern = r\"@\\w+\"\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(url_pattern, \"\", text)\n",
    "    text = re.sub(mention_pattern, \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text) \n",
    "    return text.strip()\n",
    "\n",
    "data[\"cleaned_text\"] = data[\"text\"].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f379486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \n",
       "0       1  our deeds are the reason of this #earthquake m...  \n",
       "1       1             forest fire near la ronge sask. canada  \n",
       "2       1  all residents asked to 'shelter in place' are ...  \n",
       "3       1  13,000 people receive #wildfires evacuation or...  \n",
       "4       1  just got sent this photo from ruby #alaska as ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d379e98",
   "metadata": {},
   "source": [
    "### Cleaned Text\n",
    "\n",
    "At this stage, we have successfully generated a clean version of the text data.  \n",
    "As a result, the original raw text column is no longer needed and can be safely removed.  \n",
    "Additionally, the **keyword** and **location** columns are not required for our modeling purposes, so we will drop these columns as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "929c1fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                       cleaned_text\n",
       "0       1  our deeds are the reason of this #earthquake m...\n",
       "1       1             forest fire near la ronge sask. canada\n",
       "2       1  all residents asked to 'shelter in place' are ...\n",
       "3       1  13,000 people receive #wildfires evacuation or...\n",
       "4       1  just got sent this photo from ruby #alaska as ..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = data.drop(columns=[\"keyword\",\"location\",\"id\",\"text\"])\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536d5d6d",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "### Train-Test Split\n",
    "\n",
    "To prepare for model training, we will divide the dataset into an **80% training set** and a **20% test set**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c4b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "X_train, X_test = train_test_split(\n",
    "    data, \n",
    "    test_size=0.2,       \n",
    "    random_state=42,      \n",
    "    stratify=data['target'] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b52048a",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "To process the natural language text for modeling, we require a tokenizer that can appropriately segment the text into individual tokens.  \n",
    "Given the nature of our data we will use the **`TweetTokenizer`** from the **NLTK** library, which is specifically designed to handle the unique characteristics of tweets, such as hashtags, mentions, and emoticons.\n",
    "\n",
    "`TweetTokenizer` is a rule based tokenizer which handles tweets well. It has built in support for emojis and common twitter slang. Despite not being pretrained, `TweetTokenizer` is recognized as an effective for tweet processing.\n",
    "\n",
    "https://www.nltk.org/api/nltk.tokenize.casual.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed8405e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(X_train,  max_len = 100):\n",
    "    tokenizer = TweetTokenizer()\n",
    "\n",
    "    tokenized_texts = [tokenizer.tokenize(text) for text in X_train['cleaned_text']]\n",
    "\n",
    "    all_tokens = [token for tokens in tokenized_texts for token in tokens]\n",
    "    vocab = {token: idx + 1 for idx, (token, _) in enumerate(Counter(all_tokens).most_common())}  # +1 because 0 will be padding\n",
    "\n",
    "    encoded_texts = [[vocab[token] for token in tokens if token in vocab] for tokens in tokenized_texts]\n",
    "\n",
    "    padded_texts = np.zeros((len(encoded_texts), max_len), dtype=int)\n",
    "    for i, seq in enumerate(encoded_texts):\n",
    "        length = min(len(seq), max_len)\n",
    "        padded_texts[i, :length] = seq[:length]\n",
    "\n",
    "    X_tensor = torch.tensor(padded_texts, dtype=torch.long)\n",
    "    y_tensor = torch.tensor(X_train['target'].values, dtype=torch.float32)\n",
    "\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    return vocab, dataloader\n",
    "\n",
    "\n",
    "vocab, dataloader = tokenize(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e23571",
   "metadata": {},
   "source": [
    "### LSTM Model Overview\n",
    "\n",
    "We use an LSTM for this task because tweets are sequential text data, and LSTMs are effective at capturing temporal relationships and dependencies between words, which improves classification performance on natural language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35af4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=64, output_dim=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        out = self.fc(hidden[-1])\n",
    "        return self.sigmoid(out).squeeze()\n",
    "    \n",
    "\n",
    "\n",
    "def train_model(model, epochs, optimizer, criterion):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model \n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model, X_test, vocab, max_len = 100, batch_size=32):\n",
    "    model.eval()\n",
    "    tokenizer = TweetTokenizer()\n",
    "    \n",
    "    tokenized_texts = [tokenizer.tokenize(text) for text in X_test['cleaned_text']]\n",
    "    encoded_texts = [[vocab[token] for token in tokens if token in vocab] for tokens in tokenized_texts]\n",
    "    \n",
    "    padded_texts = np.zeros((len(encoded_texts), max_len), dtype=int)\n",
    "    for i, seq in enumerate(encoded_texts):\n",
    "        length = min(len(seq), max_len)\n",
    "        padded_texts[i, :length] = seq[:length]\n",
    "\n",
    "    X_tensor = torch.tensor(padded_texts, dtype=torch.long)\n",
    "    y_tensor = torch.tensor(X_test['target'].values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    test_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9110ba1",
   "metadata": {},
   "source": [
    "### Initial Model Performance\n",
    "\n",
    "Our initial model shows a poor performance of 42.9%. We will attempt hyperparamter tuning to increase our model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e4094b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.4294156270518713\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(vocab_size = len(vocab), hidden_dim=64)\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "model = train_model(model=model,epochs=5, criterion=criterion, optimizer=optimizer)\n",
    "\n",
    "\n",
    "acc = evaluate(model, X_test, vocab)\n",
    "print(\"Model accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96032b5e",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "Changing the optimizer from **SGD** to **Adam** resulted in an increase in test accuracy. This suggests that, for this specific model and dataset, **Adam** may be better suited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9fa245c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.5705843729481287\n"
     ]
    }
   ],
   "source": [
    "vocab, dataloader = tokenize(X_train)\n",
    "\n",
    "model = LSTM(vocab_size = len(vocab), hidden_dim=64)\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "model = train_model(model=model,epochs=1, criterion=criterion, optimizer=optimizer)\n",
    "\n",
    "\n",
    "acc = evaluate(model, X_test, vocab)\n",
    "print(\"Model accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3caa04",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Our hyperparameter tuning efforts resulted in a modest improvement in model performance.  \n",
    "To achieve further gains, future work would likely need to focus on either modifying the dataset — such as through additional preprocessing or feature engineering — or making fundamental changes to the model architecture itself to better capture the underlying patterns in the data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
